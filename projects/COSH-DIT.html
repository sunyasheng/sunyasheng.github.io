<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Cosh-DiT: Co-Speech Gesture Video Synthesis via Hybrid Audio-Visual Diffusion Transformers</title>

<!-- Meta tags for search engines to crawl -->
<meta name="robots" content="index,follow">
<meta name="description" content="
">
<meta name="keywords" content="Lip-Sync; Deep learning;">
<link rel="author" href="https://hangz-nju-cuhk.github.io/">

<!-- Fonts and stuff -->
<link href="./AV-CAT/css" rel="stylesheet" type="text/css">
<link rel="stylesheet" type="text/css" href="./AV-CAT/project.css" media="screen">
<link rel="stylesheet" type="text/css" media="screen" href="./AV-CAT/iconize.css">
<script async="" src="./AV-CAT/prettify.js"></script>


</head>

<body>
  <div id="content">
    <div id="content-inner">

      <div class="section head">
	<h1>Cosh-DiT: Co-Speech Gesture Video Synthesis via
		<br>
		Hybrid Audio-Visual Diffusion Transformers</h1>

	<div class="authors">
	<a href="https://scholar.google.com/citations?user=Vrq1yOEAAAAJ">Yasheng Sun</a><sup>1,3*</sup>&nbsp;&nbsp;
    <a href="https://seanseattle.github.io/">Zhiliang Xu</a><sup>2*</sup>&nbsp;&nbsp;
	<a href="https://hangz-nju-cuhk.github.io/">Hang Zhou</a><sup>2</sup>&nbsp;&nbsp;
	<a href="https://scholar.google.com/citations?user=LocNpywAAAAJ&hl=en&oi=ao">Jiazhi Guan</a><sup>4</sup>&nbsp;&nbsp;
		<!-- <a href="https://wuqianyi.top/">Qianyi Wu</a><sup>4</sup>&nbsp;&nbsp; -->

	<!-- <a href="https://scholar.google.com.au/citations?user=9IIxWBsAAAAJ">Zhibin Hong</a><sup>2</sup>&nbsp;&nbsp; -->

	Quanwei Yang<sup>5</sup>&nbsp;&nbsp;

	<br>
	<a href="https://scholar.google.com/citations?user=2Pedf3EAAAAJ&hl=en">Kaisiyuan Wang</a><sup>2</sup>&nbsp;&nbsp;
	<a href="https://scholar.google.com/citations?user=miLIl2gAAAAJ&hl=en&oi=ao">Borong Liang</a><sup>2</sup>&nbsp;&nbsp;
	Yingying Li<sup>2</sup>&nbsp;&nbsp;
	<a href="https://scholar.google.com/citations?user=pnuQ5UsAAAAJ&hl=en&oi=ao">Haocheng Feng</a><sup>2</sup>&nbsp;&nbsp;


	<!-- <a href="https://scholar.google.com/citations?user=tVV3jmcAAAAJ">Jingtuo Liu<sup>2</sup>&nbsp;&nbsp;
	<a href="https://scholar.google.com/citations?user=1wzEtxcAAAAJ">Errui Ding</a><sup>2</sup>&nbsp;&nbsp; -->
	<a href="https://jingdongwang2017.github.io/">Jingdong Wang</a><sup>2</sup>&nbsp;&nbsp;
		<a href="https://liuziwei7.github.io/">Ziwei Liu</a><sup>6</sup>&nbsp;&nbsp;
		<a href="https://www.vogue.cs.titech.ac.jp/koike">Koike Hideki</a><sup>1</sup>

	</div>

	<div class="affiliations">
	1. Center of Excellence for Generative AI, King Abdullah University of Science and Technology	
  <br>
	2. Department of Computer Vision Technology (VIS), Baidu Inc.,
  <br>
	3. Tokyo Institute of Technology,
  <br>
	4. Department of Computer Science and Technology, Tsinghua University,
  <br>
	5. Department of Electronic Engineering and Information Science, University of Science and Technology of China,
  <br>
	6. S-Lab, Nanyang Technological University.
  </div>

	<!-- <div class="venue"> <a href="https://sa2022.siggraph.org/en/" target="_blank">SIGGRAPH Asia  2022</a> (Conference Proceedings) </div>
      </div> -->

      <center><img src="./COSH-DIT/COSH-DIT.png" border="0" width="90%"></center>

<div class="section abstract">
	<h2>Abstract</h2>
	<br>
	<p>
		Co-speech gesture video synthesis is a challenging task that requires both probabilistic modeling of human gestures and the synthesis of realistic images that align with the rhythmic nuances of speech. To address these challenges, we propose Cosh-DiT, a Co-speech gesture video system with hybrid Diffusion Transformers that perform audio-to-motion and motion-to-video synthesis using discrete and continuous diffusion modeling, respectively.  
First, we introduce an audio Diffusion Transformer (Cosh-DiT-A) to synthesize expressive gesture dynamics synchronized with speech rhythms. To capture upper body, facial, and hand movement priors, we employ vector-quantized variational autoencoders (VQ-VAEs) to jointly learn their dependencies within a discrete latent space. Then, for realistic video synthesis conditioned on the generated speech-driven motion, we design a visual Diffusion Transformer (Cosh-DiT-V) that effectively integrates spatial and temporal contexts. Extensive experiments demonstrate that our framework consistently generates lifelike videos with expressive facial expressions and natural, smooth gestures that align seamlessly with speech.	</p>
      </div>

<div class="section demo">
	<h2>Demo Video</h2>
	<br>
	<center>
	  <iframe width="640" height="360" src="https://youtu.be/suyoiSz2hoc" frameborder="0" allowfullscreen></iframe>
	</video>
	    </center>
	    </div>

<br>

<div class="section materials">
	<h2>Materials</h2>
	<center>
	  <ul>

          <li class="grid">
	      <div class="griditem">
		<a href="https://arxiv.org/abs/2503.09942" target="_blank" class="imageLink"><img src="./COSH-DIT/paper.png"></a><br>
		  <a href="https://arxiv.org/abs/2503.09942" target="_blank">Paper</a>
		</div>
	      </li>

	    </ul>
	    </center>
	    </div>

<br>



<br>

<div class="section citation">
	<h2>Citation</h2>
	<div class="section bibtex">
	  <pre>
<!-- @inproceedings{sun2022masked,
author = {Sun, Yasheng and Zhou, Hang and Wang, Kaisiyuan and Wu, Qianyi and Hong, Zhibin and Liu, Jingtuo and Ding, Errui and Wang, Jingdong and Liu, Ziwei and Hideki, Koike},
title = {Masked Lip-Sync Prediction by Audio-Visual Contextual Exploitation in Transformers},
year = {2022},
series = {SA '22 Conference Papers}
} -->
@misc{https://doi.org/10.48550/arxiv.2302.06857,
	<!-- doi = {10.48550/ARXIV.2302.06857}, -->
	<!-- url = {https://arxiv.org/abs/2302.06857}, -->
	author = {Yasheng Sun, Zhiliang Xu, Hang Zhou, Jiazhi Guan, Quanwei Yang, Kaisiyuan Wang, Borong Liang, Yingying Li, Haocheng Feng, Jingdong Wang, Ziwei Liu, Koike Hideki},
	title = {Cosh-DiT: Co-Speech Gesture Video Synthesis via Hybrid Audio-Visual Diffusion Transformers},
	publisher = {arXiv},
	year = {2025},
	copyright = {Creative Commons Attribution 4.0 International}
  }
</pre>
<br>

	  </div>
      </div>

</body></html>
